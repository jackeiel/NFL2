norm.train <- predict(norm.values, treated.train)
norm.test <- predict(norm.values, treated.test)
norm.test$accepted = ifelse(treated.test$Y_AcceptedOffer=='Accepted', 1, 0)
norm.test$notAccepted = ifelse(treated.test$Y_AcceptedOffer=='DidNotAccept', 1, 0)
norm.test$Y_AcceptedOffer=NULL
norm.train$accepted = ifelse(treated.train$Y_AcceptedOffer=='Accepted', 1, 0)
norm.train$notAccepted = ifelse(treated.train$Y_AcceptedOffer=='DidNotAccept', 1, 0)
norm.train$Y_AcceptedOffer=NULL
net = neuralnet(accepted+notAccepted~., data=norm.train, hidden=c(2), linear.output = F)
norm.test.reduced = norm.test[,1:63]
predictions = predict(net, norm.test, rep=1)
predicted.class <- ifelse(predictions[,1]>0.5, 'Accepted', 'DidNotAccept')
confusionMatrix(as.factor(predicted.class), treated.test$Y_AcceptedOffer)
set.seed(10)
tree = rpart(Y_AcceptedOffer ~ .,
data = trainData,
method = "class",
minsplit = 5,
xval=5,
cp=0.0001)
best.cp = tree$cptable[which.min(tree$cptable[,4]), 1]
best.tree = rpart(Y_AcceptedOffer~., data=trainData, method='class',
cp=best.cp, minsplit=5)
tree.predict = predict(tree, testData, type='class')
best.tree.predict = predict(best.tree, testData, type='class')
conf.tree = confusionMatrix(tree.predict, testData$Y_AcceptedOffer)
conf.best.tree = confusionMatrix(best.tree.predict, testData$Y_AcceptedOffer)
print('accuracy of unpruned tree')
sum(diag(conf.tree$table)) / sum(conf.tree$table)
print('accuracy of pruned tree')
sum(diag(conf.best.tree$table)) / sum(conf.best.tree$table)
prp(best.tree)
# varImp(best.tree)
v = varImp(best.tree)
v$Variable = row.names(v)
t = subset(v, v$Overall>15)
desc.order = order(t$Overall, decreasing = T)
ggplot(t)+
geom_bar(aes(x=Variable , y=Overall, fill = Variable), stat='Identity') +
theme(axis.text.x = element_blank()) +
ylab('Variable Importance')
library(MLmetrics)
set.seed(10)
n_trees = NA
accuracy = NA
row = 1
#I saved the model so I don't have to re-reun this every time I knit RMD file.
if (! file.exists('best_randomforest.rds')){
for (i in seq(100, 300, 50)){
# Fit a random forest model with Caret
rf <- train(Y_AcceptedOffer ~ .,
data = treated.train,
method = "rf",
verbose = FALSE,
ntree = i,
tuneGrid = data.frame(mtry = 1))
n_trees[row] = i
predictions = predict(rf, treated.test)
accuracy[row] = Accuracy(treated.test$Y_AcceptedOffer, predictions)
row = row + 1
}
best.ntree = n_trees[order(accuracy, decreasing=T)[1]]
best.rf = train(Y_AcceptedOffer~.,
data=treated.train,
method='rf',
verbose=F,
ntree=best.ntree,
tuneGrid = data.frame(mtry = 1))
saveRDS(best.rf, 'best_randomforest.rds')
}
if (file.exists('best_randomforest.rds')){
best.rf = readRDS('best_randomforest.rds')
}
predictions.train.rf = predict(best.rf, treated.train)
names(treated.train)
names(treated.train1)
knitr::opts_chunk$set(echo = TRUE)
# just to clear the environment
rm(list=ls())
setwd("~/Harvard/DataMining/Case2")
library(dplyr)
library(vtreat)
library(caret)
library(DataExplorer)
library(rpart)
library(rpart.plot)
# Raw data, need to add others
cust   <- read.csv('CurrentCustomerMktgResults.csv')
veh <- read.csv('householdVehicleData.csv')
ax = read.csv('householdAxiomData.csv')
cred = read.csv('householdCreditData.csv')
# Perform a join, neeed to add other data sets
joinData = left_join(cust, veh, by = c('HHuniqueID'))
joinData <- left_join(joinData, ax, by = c('HHuniqueID'))
data <- left_join(joinData, cred, by=c('HHuniqueID'))
data$dataID=NULL
head(data)
anyDuplicated(data)
anyDuplicated(data$HHuniqueID)
plot_missing(data)
data$past_Outcome = NULL
unique(data$Communication)
unique(data$carYr)
sum(is.na(data$carModel))
#note NAs
data$carYr[6:60]
na.carYr = data[is.na(data$carYr),]
na.carModels = NA
for (i in 1:nrow(na.carYr)){
#we want to find the rounded down average year of each of these vehicles
model = na.carYr$carModel[i]
x = subset(data, carModel==model)
y = floor(mean(na.omit(x$carYr)))
na.carModels[i] = y
}
data[is.na(data$carYr),'carYr'] = na.carModels
data$carYr[6:60]
data[is.na(data$carYr),]
x = is.na(data$carYr)
na.data = data[x,]
na.data
#cannot use data[-na.data,] to subset this data for some reason, used this shortcut. Also removing variable X , only value listed is accepted, and communication due to it's
clean.data = data[-c(314,911,2149,2212,2699,3133,3334),-c(1,2,11)]
clean.data$CallEnd = NULL
clean.data$CallStart = NULL
clean.data$carModel = NULL
plot_missing(clean.data)
sapply(clean.data, class)
set.seed(1234)
nTrain = round(nrow(clean.data)*.60)
total = nrow(clean.data)
idx = sample(total, nTrain)
trainData = clean.data[idx,]
testData = clean.data[-idx,]
library(vtreat)
plan = designTreatmentsC(trainData, names(clean.data)[-6], 'Y_AcceptedOffer', 'Accepted')
treated.train1 = prepare(plan, trainData)
treated.test1 = prepare(plan, testData)
vars.toDrop = c(2,3,7,8,10, 11, 12,13,18:23)
treated.train = treated.train1[,-vars.toDrop]
treated.test = treated.test1[,-vars.toDrop]
names(treated.train)
names(treated.train1)
names(trainData)
plan
saveRDS(net, './neural_net.rds')
ggplot()+
geom_histogram(data=clean.data, aes(x=Age, fill=Y_AcceptedOffer))
ggplot()+
geom_histogram(data=clean.data, aes(x=Age, fill=Y_AcceptedOffer))+
facet_wrap(~Y_AcceptedOffer)
2000/60
knitr::opts_chunk$set(echo = TRUE)
probs = read.csv('prediction_prob.csv')
cust = read.csv('ProspectiveCustomers.csv')
veh <- read.csv('householdVehicleData.csv')
ax = read.csv('householdAxiomData.csv')
cred = read.csv('householdCreditData.csv')
# Perform a join, neeed to add other data sets
joinData = left_join(cust, veh, by = c('HHuniqueID'))
joinData <- left_join(joinData, ax, by = c('HHuniqueID'))
joinData <- left_join(joinData, cred, by=c('HHuniqueID'))
probs$X = NULL
probs$HHuniqueID = probs$prospects.HHuniqueID
probs$prospects.HHuniqueID=NULL
data = left_join(joinData, probs, by=c('HHuniqueID'))
data$PredClass = ifelse(data$prospectPreds>0.5, 'Accepted', 'DidNotAccept')
dim(data)
head(data)
sorted_probs = order(data$prospectPreds, decreasing = T)
top100 = data[sorted_probs[1:100],]
low_sorted_probs = order(data$prospectPreds, decreasing = )
not100 = data[low_sorted_probs[1:100],]
library(ggplot2)
ggplot(top100) +
geom_histogram(aes(x=Age), bins=15) +
facet_wrap(~Marital)
ggplot(not100) +
geom_histogram(aes(x=Age), bins=15) +
facet_wrap(~Marital)
sub = subset(data, data$RecentBalance<20000)
ggplot(sub)+
geom_histogram(aes(x=RecentBalance, fill=PredClass), bins=15) +
# geom_histogram(aes(x=RecentBalance, fill=prospectPreds), bins=15) +
facet_wrap(~PredClass)
ggplot()+
geom_point(data=top100, aes(x=Age, y=RecentBalance, color=PredClass))+
geom_point(data=not100, aes(x=Age, y=RecentBalance, color=PredClass))
ggplot()+
geom_density(data=not100, aes(x=Age, fill=PredClass))+
geom_density(data=top100, aes(x=Age, fill=PredClass, alpha=0.4))+
geom_vline(data=not100, aes(xintercept=mean(Age)), color='lightblue', linetype='dashed')
ggplot()+
geom_point(data=top100, aes(x=Age, y=LastContactDay, color=PredClass)) +
geom_point(data=not100, aes(x=Age, y=LastContactDay, color=PredClass))
ggplot()+
geom_bar(data=top100, aes(x=Communication, fill=PredClass))+
geom_bar(data=not100, aes(x=Communication, fill=PredClass))+
facet_wrap(~PredClass)
ggplot()+
geom_bar(data=top100, aes(x=Marital, fill=PredClass))+
geom_bar(data=not100, aes(x=Marital, fill=PredClass))+
facet_wrap(~PredClass)
ggplot()+
geom_point(data=top100, aes(x=NoOfContacts, y=PrevAttempts, color=PredClass))+
geom_point(data=not100, aes(x=NoOfContacts, y = PrevAttempts, color=PredClass))
ggplot()+
geom_bar(data=top100, aes(x=HHInsurance, fill=CarLoan))+
geom_bar(data=not100, aes(x=HHInsurance, fill=CarLoan))+
facet_wrap(~PredClass)
ggplot()+
geom_bar(data=top100, aes(x=Education))+
geom_bar(data=not100, aes(x=Education))+
facet_wrap(~PredClass)
ggplot()+
geom_density(data=not100, aes(x=Age, fill=PredClass))+
geom_density(data=top100, aes(x=Age, fill=PredClass, alpha=0.9))
sorted_probs = order(data$prospectPreds, decreasing = T)
top = data[sorted_probs,]
low_sorted_probs = order(data$prospectPreds, decreasing = )
not100 = data[low_sorted_probs[1:100],]
library(ggplot2)
ggplot(data) +
geom_histogram(aes(x=Age), bins=15) +
facet_wrap(~Marital)
ggplot(not100) +
geom_histogram(aes(x=Age), bins=15) +
facet_wrap(~Marital)
sub = subset(data, data$RecentBalance<20000)
ggplot(sub)+
geom_histogram(aes(x=RecentBalance, fill=PredClass), bins=15) +
# geom_histogram(aes(x=RecentBalance, fill=prospectPreds), bins=15) +
facet_wrap(~PredClass)
ggplot()+
geom_point(data=top100, aes(x=Age, y=RecentBalance, color=PredClass))+
geom_point(data=not100, aes(x=Age, y=RecentBalance, color=PredClass))
ggplot()+
geom_density(data=not100, aes(x=Age, fill=PredClass))+
geom_density(data=top100, aes(x=Age, fill=PredClass, alpha=0.4))+
geom_vline(data=not100, aes(xintercept=mean(Age)), color='lightblue', linetype='dashed')
ggplot()+
geom_point(data=top100, aes(x=Age, y=LastContactDay, color=PredClass)) +
geom_point(data=not100, aes(x=Age, y=LastContactDay, color=PredClass))
ggplot()+
geom_bar(data=top100, aes(x=Communication, fill=PredClass))+
geom_bar(data=not100, aes(x=Communication, fill=PredClass))+
facet_wrap(~PredClass)
ggplot()+
geom_bar(data=top100, aes(x=Marital, fill=PredClass))+
geom_bar(data=not100, aes(x=Marital, fill=PredClass))+
facet_wrap(~PredClass)
ggplot()+
geom_point(data=top100, aes(x=NoOfContacts, y=PrevAttempts, color=PredClass))+
geom_point(data=not100, aes(x=NoOfContacts, y = PrevAttempts, color=PredClass))
ggplot()+
geom_bar(data=top100, aes(x=HHInsurance, fill=as.factor(CarLoan)))+
geom_bar(data=not100, aes(x=HHInsurance, fill=as.factor(CarLoan)))+
facet_wrap(~PredClass)
ggplot()+
geom_bar(data=top100, aes(x=Education))+
geom_bar(data=not100, aes(x=Education))+
facet_wrap(~PredClass)
ggplot()+
geom_histogram(data=top100, aes(x=Age, fill=headOfHouseholdGender))+
facet_wrap(~headOfHouseholdGender)
names(top100)
ggplot()+
geom_histogram(data=top100, aes(x=Age, fill=headOfhouseholdGender))+
facet_wrap(~headOfhouseholdGender)
ggplot()+
geom_histogram(data=top100, aes(x=Age, fill=headOfhouseholdGender), bins=15)+
facet_wrap(~headOfhouseholdGender)
ggplot()+
geom_bar(data=top100, aes(x=HHInsurance, fill=as.factor(CarLoan)))+
geom_bar(data=not100, aes(x=HHInsurance, fill=as.factor(CarLoan)))+
facet_wrap(~PredClass)
mean(top100$Age)
mean(not100$Age)
ggplot()+
geom_bar(data=top100, aes(x=HHInsurance, fill=PredClass))+
geom_bar(data=not100, aes(x=HHInsurance, fill=PredClass))+
facet_wrap(~PredClass)
names(top100)
ggplot()+
geom_density(data=top100, aes(x=carYr))+
geom_density(data=not100, aes(x=carYr))+
facet_wrap(~PredClass)
ggplot()+
geom_density(data=top100, aes(x=carYr, fill=PredClass))+
geom_density(data=not100, aes(x=carYr, fill=PredClass))+
facet_wrap(~PredClass)
ggplot()+
geom_point(data=top100, aes(x=carYr, y=RecentBalance))
ggplot()+
geom_point(data=top100, aes(x=carYr, y=RecentBalance, color=PredClass))+
geom_point(data=not100, aes(x=carYr, y=RecentBalance, color=PredClass))
ggplot()+
geom_bar(data=top100, aes(x=DigitalHabits_5_AlwaysOn))
ggplot(data)+
geom_bar(aes(x=carMake, fill=PredClass)
ggplot(data)+
geom_bar(aes(x=carMake, fill=PredClass))
ggplot(data)+
geom_bar(aes(x=carMake, fill=PredClass))+
coord_flip()
ggplot(data)+
geom_density(aes(x=Age, group=PredClass))+
ggplot(data)+
geom_density(aes(x=Age, group=PredClass))
ggplot(data)+
geom_density(aes(x=Age, group=PredClass, fill=PredClass))
ggplot(data)+
geom_density(aes(x=Age, group=PredClass, fill=PredClass, alpha=0.4))
ggplot(data)+
geom_density(aes(x=Age, group=PredClass, fill=PredClass), alpha=0.4
ggplot(data)+
geom_density(aes(x=Age, group=PredClass, fill=PredClass), alpha=0.4)
ggplot(data)+
geom_bar(aes(x=HHInsurance))+
facet_wrap(~PredClass)
knitr::opts_chunk$set(echo = TRUE)
# just to clear the environment
rm(list=ls())
setwd("~/Harvard/DataMining/Case2")
library(dplyr)
library(vtreat)
library(caret)
library(DataExplorer)
library(rpart)
library(rpart.plot)
library(MLmetrics)
# Raw data, need to add others
cust   <- read.csv('CurrentCustomerMktgResults.csv')
veh <- read.csv('householdVehicleData.csv')
ax = read.csv('householdAxiomData.csv')
cred = read.csv('householdCreditData.csv')
# Perform a join, neeed to add other data sets
joinData = left_join(cust, veh, by = c('HHuniqueID'))
joinData <- left_join(joinData, ax, by = c('HHuniqueID'))
data <- left_join(joinData, cred, by=c('HHuniqueID'))
data$dataID=NULL
head(data)
anyDuplicated(data)
anyDuplicated(data$HHuniqueID)
plot_missing(data)
data$past_Outcome = NULL
unique(data$Communication)
unique(data$carYr)
sum(is.na(data$carModel))
#note NAs
data$carYr[6:60]
na.carYr = data[is.na(data$carYr),]
na.carModels = NA
for (i in 1:nrow(na.carYr)){
#we want to find the rounded down average year of each of these vehicles
model = na.carYr$carModel[i]
x = subset(data, carModel==model)
y = floor(mean(na.omit(x$carYr)))
na.carModels[i] = y
}
data[is.na(data$carYr),'carYr'] = na.carModels
data$carYr[6:60]
data[is.na(data$carYr),]
x = is.na(data$carYr)
na.data = data[x,]
na.data
#cannot use data[-na.data,] to subset this data for some reason, used this shortcut. Also removing variable X , only value listed is accepted, and communication due to it's
clean.data = data[-c(314,911,2149,2212,2699,3133,3334),-c(1,2,11)]
clean.data$CallEnd = NULL
clean.data$CallStart = NULL
clean.data$carModel = NULL
plot_missing(clean.data)
sapply(clean.data, class)
set.seed(1234)
nTrain = round(nrow(clean.data)*.60)
total = nrow(clean.data)
idx = sample(total, nTrain)
trainData = clean.data[idx,]
testData = clean.data[-idx,]
library(vtreat)
plan = designTreatmentsC(trainData, names(clean.data)[-6], 'Y_AcceptedOffer', 'Accepted')
treated.train1 = prepare(plan, trainData)
treated.test1 = prepare(plan, testData)
vars.toDrop = c(2,3,7,8,10, 11, 12,13,18:23)
treated.train = treated.train1[,-vars.toDrop]
treated.test = treated.test1[,-vars.toDrop]
set.seed(10)
log.train = treated.train
log.train$Y_AcceptedOffer = ifelse(log.train$Y_AcceptedOffer=='Accepted', 1, 0)
log.test = treated.test
log.test$Y_AcceptedOffer = ifelse(log.test$Y_AcceptedOffer=='Accepted', 1, 0)
log.fit = glm(Y_AcceptedOffer~., data = log.train, family='binomial')
if(! file.exists('log_back.rds')){
log.back = step(log.fit, direction='backward', trace = F)
log.null = glm(Y_AcceptedOffer~1, data=log.train, family = 'binomial')
log.forward =step(log.null, direction = 'forward', trace=F)
}
if (file.exists('log_back.rds')){
log.back = readRDS('log_back.rds')
log.forward = readRDS('log_forward.rds')
}
backPredictions = predict(log.back, treated.test[,-64], type=c('response'))
forwardPredictions = predict(log.forward, treated.test[,-64], type=c('response'))
log.backPredictions = ifelse(backPredictions>0.5, 1, 0)
log.forwardPredictions = ifelse(forwardPredictions>0.5, 1, 0)
confusion_back = confusionMatrix(as.factor(log.backPredictions), as.factor(log.test$Y_AcceptedOffer))
confusion_forward = confusionMatrix(as.factor(log.forwardPredictions), as.factor(log.test$Y_AcceptedOffer))
print('accuracy of backward model')
sum(diag(confusion_back$table)) / sum(confusion_back$table)
print('accuracy of forward model')
sum(diag(confusion_forward$table)) / sum(confusion_forward$table)
saveRDS(log.back, './log_back.rds')
library(neuralnet)
#first step is to normalize
norm.values <- preProcess(treated.train, method="range")
norm.train <- predict(norm.values, treated.train)
norm.test <- predict(norm.values, treated.test)
norm.test$accepted = ifelse(treated.test$Y_AcceptedOffer=='Accepted', 1, 0)
norm.test$notAccepted = ifelse(treated.test$Y_AcceptedOffer=='DidNotAccept', 1, 0)
norm.test$Y_AcceptedOffer=NULL
norm.train$accepted = ifelse(treated.train$Y_AcceptedOffer=='Accepted', 1, 0)
norm.train$notAccepted = ifelse(treated.train$Y_AcceptedOffer=='DidNotAccept', 1, 0)
norm.train$Y_AcceptedOffer=NULL
net = neuralnet(accepted+notAccepted~., data=norm.train, hidden=c(2), linear.output = F)
norm.test.reduced = norm.test[,1:63]
predictions = predict(net, norm.test, rep=1)
predicted.class <- ifelse(predictions[,1]>0.5, 'Accepted', 'DidNotAccept')
confusionMatrix(as.factor(predicted.class), treated.test$Y_AcceptedOffer)
set.seed(10)
tree = rpart(Y_AcceptedOffer ~ .,
data = trainData,
method = "class",
minsplit = 5,
xval=5,
cp=0.0001)
best.cp = tree$cptable[which.min(tree$cptable[,4]), 1]
best.tree = rpart(Y_AcceptedOffer~., data=trainData, method='class',
cp=best.cp, minsplit=5)
tree.predict = predict(tree, testData, type='class')
best.tree.predict = predict(best.tree, testData, type='class')
conf.tree = confusionMatrix(tree.predict, testData$Y_AcceptedOffer)
conf.best.tree = confusionMatrix(best.tree.predict, testData$Y_AcceptedOffer)
print('accuracy of unpruned tree')
sum(diag(conf.tree$table)) / sum(conf.tree$table)
print('accuracy of pruned tree')
sum(diag(conf.best.tree$table)) / sum(conf.best.tree$table)
prp(best.tree)
# varImp(best.tree)
v = varImp(best.tree)
v$Variable = row.names(v)
t = subset(v, v$Overall>15)
desc.order = order(t$Overall, decreasing = T)
ggplot(t)+
geom_bar(aes(x=Variable , y=Overall, fill = Variable), stat='Identity') +
theme(axis.text.x = element_blank()) +
ylab('Variable Importance')
set.seed(1234)
n_trees = NA
accuracy = NA
row = 1
#I saved the model so I don't have to re-reun this every time I knit RMD file.
if (! file.exists('best_randomforest.rds')){
for (i in seq(100, 300, 50)){
# Fit a random forest model with Caret
rf <- train(Y_AcceptedOffer ~ .,
data = treated.train,
method = "rf",
verbose = FALSE,
ntree = i,
tuneGrid = data.frame(mtry = 1))
n_trees[row] = i
predictions = predict(rf, treated.test)
accuracy[row] = Accuracy(treated.test$Y_AcceptedOffer, predictions)
row = row + 1
}
best.ntree = n_trees[order(accuracy, decreasing=T)[1]]
best.rf = train(Y_AcceptedOffer~.,
data=treated.train,
method='rf',
verbose=F,
ntree=best.ntree,
tuneGrid = data.frame(mtry = 1))
saveRDS(best.rf, 'best_randomforest.rds')
}
if (file.exists('best_randomforest.rds')){
best.rf = readRDS('best_randomforest.rds')
}
predictions.train.rf = predict(best.rf, treated.train)
setwd("~/Python/NFL2/DATA/play_by_play_data/regular_season")
install.packages('devtools')
devtools::install_github(repo = "maksimhorowitz/nflscrapR")
ids = scrape_game_ids(2019)
library(nflscrapR)
ids = scrape_game_ids(2019)
ids
non = data.frame()
rbind(non,ids)
write.csv(pbp, '../play_by_play_data/regular_season/reg_pbp_2019.csv')
library(nflscrapR)
ids = scrape_game_ids(2019)
week = 1
week_games = ids[ids$week==week,]
week_games
pbp = data.frame()
for (game in week_games$game_id){
pbp = rbind(pbp, scrape_game_play_by_play(game, 'reg', 2019))
}
write.csv(pbp, '../play_by_play_data/regular_season/reg_pbp_2019.csv')
write.csv(pbp, './play_by_play_data/regular_season/reg_pbp_2019.csv')
setwd("~/Python/NFL2/DATA/R")
write.csv(pbp, './play_by_play_data/regular_season/reg_pbp_2019.csv')
write.csv(pbp, '../play_by_play_data/regular_season/reg_pbp_2019.csv')
